# -*- coding: utf-8 -*-
"""mergingData.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ogb2IxoB2XZLvXG80nVrpr9vaLf7Q08J

Bik to CSV
"""

import pandas as pd
import requests
import json

csvTable = pd.read_table('Bik_dataset-papers_with_endpoint_reached.tsv', sep = '\t', encoding = "ISO-8859-1")
#pd.DataFrame(csvTable)

df = pd.DataFrame(csvTable[0:214])

"""# Lab size"""

findingsData = pd.read_json('findings.json')

labs = []
for col in findingsData:
  try:
    labs.append(findingsData[col][1][0])
  except:
    labs.append('')

df['Labs'] = labs

labsData = pd.read_json('labs.json')

index = 0
labSizes = []
for index in range(len(df)):
  currentLab = df["Labs"][index]
  if currentLab != '':
    labSizes.append(labsData[currentLab][0])
  else:
    labSizes.append('')

df['Lab Size'] = labSizes

"""# Publication Rate"""

f = open('authors.json')
authorsData = json.load(f)

nPublications = []

index = 0
for index in range(len(df)):
  authors = df['Authors'][index]
  splitAuthors = authors.split(", ")
  currentPubString = ''
  for author in splitAuthors:
    try:
      currentPubString += str(authorsData[author]['author_info'][author]['nbr_pubs']) + ' '
    except:
      currentPubString += ' '
  nPublications.append(currentPubString)

df['nPublications'] = nPublications

"""# Other Journals"""

otherJournals = []
index = 0
for index in range(len(df)):
  authors = df['Authors'][index]
  splitAuthors = authors.split(", ")
  currentJournalString = ''
  for author in splitAuthors:
    try:
      for i in range(len(authorsData[author]['author_info'][author]['published_journals'])):
        currentJournalString += authorsData[author]['author_info'][author]['published_journals'][i] + ' '
    except:
      currentJournalString += ' '
  otherJournals.append(currentJournalString)

df["Other Journals"] = otherJournals

"""# University (first author)"""

f = open('Affiliation.json')
affiliationData = json.load(f)

university = []
for i in range(len(affiliationData)):
  
  university.append(list(affiliationData[i].values())[0].strip())

df['University'] = university

"""# Career Duration (first author)"""

f = open('Career_Duration.json')
careerData = json.load(f)

careerLengths = []
for i in range(len(careerData)):
  careerLengths.append(list(careerData[i].values())[0])

df['Career Duration'] = careerLengths

"""# Highest Degree Obtained (pending)

# Degree Area
"""

f = open('Degree_Area.json')
degreeData = json.load(f)

degreeAreas = []
for i in range(len(degreeData)):
  degreeAreas.append(list(degreeData[i].values())[0])

df["Degree Area"] = degreeAreas

"""# Dataset 1"""

with open('universityData (2).json', encoding = 'utf-8') as inputfile:
  uniData = pd.read_json(inputfile)

uniData = uniData.rename(columns = {'Institution':'University'})

df = pd.merge(df, uniData, how = "left", on =["University", "University"])

"""# Dataset 2"""

with open('universities_ranking.json', encoding = 'utf-8') as inputfile:
  rankingData = pd.read_json(inputfile)

rankingData = rankingData[['title', 'ranking', 'students staff ratio', 'perc intl students']]

rankingData = rankingData.rename(columns = {'title':'University'})

df = pd.merge(df, rankingData, how = "left", on=["University", "University"])

"""# Dataset 3"""

from math import sin, cos, sqrt, atan2, radians
key = ''#fa8ef19415fe4ffb8b9432730d61bcf8' --please minimize the use of this key
universities = list(df["University"])
uni_city = {}

with open('citycrimestats.json') as json_file:
    citydata = json.load(json_file)
cities = list(citydata.keys())
output = []

for university in universities:
    try:
        r = requests.get("https://api.opencagedata.com/geocode/v1/json?q="+university+"&key="+key)
        lat = r.json()['results'][0]['geometry']['lat']
        lng = r.json()['results'][0]['geometry']['lng']
        best = 999999
        uni_city[university] = {'lat': lat, 'lng': lng}
        for city in cities:
            # approximate radius of earth in km to calculate nearst city/university
            R = 6373.0
            lat1 = radians(lat)
            lon1 = radians(lng)
            lat2 = radians(citydata[city]['city_crime_lat'])
            lon2 = radians(citydata[city]['city_crime_lng'])
            dlon = lon2 - lon1
            dlat = lat2 - lat1
            a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2
            c = 2 * atan2(sqrt(a), sqrt(1 - a))
            distance = R * c
            if distance < best:
                best_city = city
                best = distance
        output.append({'University': university, 'near_city': best_city, 'crime_index': citydata[best_city]['crime_index'], 
                         'safety_index': citydata[best_city]['safety_index']})
    
    except:
        output.append({'University': university})

citycrimestats = pd.DataFrame(output)

citycrimestats = citycrimestats[['University', 'near_city', 'crime_index', 'safety_index']]

df = pd.merge(df, citycrimestats, how = "left", on=["University", "University"])

#backup uni_city locations to reduce API use
with open("uni_locations.json", "w") as outfile:
    json.dump(uni_city, outfile)

# converting dataframe to tsv:
df.to_csv('merged_data.tsv', sep="\t")